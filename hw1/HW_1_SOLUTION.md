Николай, привет! Оставил скрины отчетов в папке hw_1_screenshots

## Думаем и проектируем

### Какие метрики важно отслеживать

Для понимания поведения системы под нагрузкой в первую очередь важно отслеживать метрики, отражающие корректность работы и пользовательский опыт

Ключевой метрикой является доля успешных запросов. Она показывает, получает ли пользователь ожидаемый ответ на свой запрос. Снижение этой доли указывает на то, что система перестаёт справляться с нагрузкой либо возникают ошибки в обработке запросов

Второй важный класс метрик — latency. Пользователю важно не только получить корректный ответ, но и получить его быстро. Для анализа задержек имеет смысл смотреть не среднее значение, а перцентили:
- p95 latency для отслеживания деградации для большинства пользователей
- p99 latency отражает поведение системы в худших сценариях и помогает выявлять редкие, но критичные задержки

Рост p99 при стабильном p95 может указывать на проблемы в отдельных запросах или узких местах системы

Для локализации узких мест также важно отслеживать метрики отдельных компонентов:
- нагрузку на backend (CPU, memory), чтобы понять, является ли он bottleneck’ом
- метрики базы данных (количество соединений, время выполнения запросов), чтобы выявить задержки на стороне PostgreSQL

### Как эти метрики помогают понять состояние системы

Если под нагрузкой падает доля успешных запросов или растёт latency, следующим шагом является анализ того, на каком этапе обработки запроса возникает задержка. Для этого необходимо сопоставлять пользовательские метрики (success rate, latency) с метриками backend и бд

Например, если latency растёт при стабильной загрузке backend, но увеличивается время работы с базой данных, это указывает на возможное узкое место в бд или взаимодействии с ней. Если же нагрузка концентрируется на backend, причиной может быть неоптимальная бизнес-логика или обработка запросов

## Стреляем, анализируем, ищем решения

### Storm

```bash
user@X16QY3GWR6-MBP demo-app-1 % k6 run --out experimental-prometheus-rw k6/scripts/load-script-storm.js

         /\      Grafana   /‾‾/  
    /\  /  \     |\  __   /  /   
   /  \/    \    | |/ /  /   ‾‾\ 
  /          \   |   (  |  (‾)  |
 / __________ \  |_|\_\  \_____/ 

     execution: local
        script: k6/scripts/load-script-storm.js
        output: -

     scenarios: (100.00%) 1 scenario, 1000 max VUs, 40s max duration (incl. graceful stop):
              * default: Up to 1000 looping VUs for 10s over 1 stages (gracefulRampDown: 30s, gracefulStop: 30s)



  █ TOTAL RESULTS 

    checks_total.......: 20239  1824.405295/s
    checks_succeeded...: 49.50% 10019 out of 20239
    checks_failed......: 50.49% 10220 out of 20239

    ✗ created
      ↳  41% — ✓ 6708 / ✗ 9550
    ✗ list ok
      ↳  83% — ✓ 3311 / ✗ 670

    HTTP
    http_req_duration..............: avg=213.38ms min=615µs   med=142.73ms max=3.13s p(90)=467.06ms p(95)=591.2ms 
      { expected_response:true }...: avg=215.2ms  min=742µs   med=112.58ms max=3.13s p(90)=491.71ms p(95)=609.25ms
    http_req_failed................: 50.49% 10220 out of 20239
    http_reqs......................: 20239  1824.405295/s

    EXECUTION
    iteration_duration.............: avg=263.67ms min=50.73ms med=192.88ms max=3.18s p(90)=517.52ms p(95)=641.38ms
    iterations.....................: 20239  1824.405295/s
    vus............................: 128    min=96             max=995 
    vus_max........................: 1000   min=1000           max=1000

    NETWORK
    data_received..................: 41 MB  3.7 MB/s
    data_sent......................: 3.5 MB 318 kB/s


running (11.1s), 0000/1000 VUs, 20239 complete and 0 interrupted iterations
default ✓ [======================================] 0000/1000 VUs  10s
```

Данный сценарий НТ приводит к тому, что система перестаёт стабильно обслуживать входящий трафик. 
Резко возрастает нагрузка на CPU (CPU Busy ≈ 100%) и load average (≈600% при 10 CPU), что указывает на формирование очереди процессов и нехватку вычислительных ресурсов на бэке

На уровне пользовательских метрик наблюдается деградация качества сервиса: около 50% HTTP-запросов завершаются ошибкой, а p99 latency достигает около 18 секунд. Массовые ошибки являются наиболее критичны, так как пользователь в этом случае не получает никакого результата, а не просто долго ожидает ответа

Также по метрикам PostgreSQL видно, что количество активных соединений с базой данных остаётся относительно небольшим и не упирается в лимиты. Это означает, что бэк не блокируется на ожидании свободных коннектов к БД, а деградация системы происходит раньше, на этапе обработки запросов и выполнения бизнес-логики. Данный факт дополнительно подтверждает, что узким местом в штормовом сценарии является именно бэк, а не бд

Для повышения устойчивости системы к такому сценарию теоретически можно:

1. Добавить rate limiting на уровне NGINX

На NGINX можно возвращать 429, сигнализируя клиенту о временной недоступности сервиса и тем самым сохранить стабильность системы вместо полной деградации

2. Кэшировать GET-запросы 

Этим можно снизить нагрузку за счёт исключения части вычислений на бэке, а бэк в данном сценарии является бутылочным горлышком

### Wave

```bash
user@X16QY3GWR6-MBP demo-app-1 % k6 run --out experimental-prometheus-rw k6/scripts/load-script.js

         /\      Grafana   /‾‾/  
    /\  /  \     |\  __   /  /   
   /  \/    \    | |/ /  /   ‾‾\ 
  /          \   |   (  |  (‾)  |
 / __________ \  |_|\_\  \_____/ 

     execution: local
        script: k6/scripts/load-script.js
        output: -

     scenarios: (100.00%) 1 scenario, 1000 max VUs, 4m30s max duration (incl. graceful stop):
              * default: Up to 1000 looping VUs for 4m0s over 4 stages (gracefulRampDown: 30s, gracefulStop: 30s)



  █ TOTAL RESULTS 

    checks_total.......: 238682 994.42411/s
    checks_succeeded...: 51.34% 122549 out of 238682
    checks_failed......: 48.65% 116133 out of 238682

    ✗ created
      ↳  42% — ✓ 81567 / ✗ 109219
    ✗ list ok
      ↳  85% — ✓ 40982 / ✗ 6914

    HTTP
    http_req_duration..............: avg=173.58ms min=581µs   med=29.23ms max=6.98s p(90)=482.81ms p(95)=849.01ms
      { expected_response:true }...: avg=86.55ms  min=711µs   med=18.76ms max=6.43s p(90)=175.87ms p(95)=417.7ms 
    http_req_failed................: 48.65% 116133 out of 238682
    http_reqs......................: 238682 994.42411/s

    EXECUTION
    iteration_duration.............: avg=223.92ms min=50.64ms med=79.53ms max=7.03s p(90)=533.42ms p(95)=899.39ms
    iterations.....................: 238682 994.42411/s
    vus............................: 1      min=1                max=998 
    vus_max........................: 1000   min=1000             max=1000

    NETWORK
    data_received..................: 506 MB 2.1 MB/s
    data_sent......................: 42 MB  173 kB/s




running (4m00.0s), 0000/1000 VUs, 238682 complete and 0 interrupted iterations
default ✓ [======================================] 0000/1000 VUs  4m0s
```

В данном система демонстрирует существенно более стабильное поведение по сравнению с штормовым сценарием

Загрузка CPU и значение load average растут постепенно и не достигают критических значений, что указывает на отсутствие лавинообразного эффекта. После прохождения пика нагрузки система корректно восстанавливается, а основные метрики возвращаются к исходным значениям

На уровне пользовательских метрик наблюдается умеренная деградация: доля ошибок составляет около 14.6%, а p99 latency достигает ~13 секунд. При этом рост задержек происходит постепенно и наблюдается его корреляция с увеличением нагрузки

БД не демонстрирует признаков перегрузки: дэдлоков нет, cache hit ratio остаётся высоким, при этом количество активных соединений к базе данных в волновом сценарии растёт плавно и коррелирует с ростом нагрузки, не достигая критических значений. После снижения нагрузки число соединений также уменьшается, что говорит о корректной работе пула соединений и отсутствии утечек. Это подтверждает, что PostgreSQL стабильно обслуживает запросы даже при пиковых значениях нагрузки и не является бутылочным горлышком в данном сценарии

Итого, при данном сценарии система способна работать вблизи своего предела при контролируемом росте нагрузки и корректно восстанавливаться после её снижения, в отличие от сценария резкого пикового воздействия
